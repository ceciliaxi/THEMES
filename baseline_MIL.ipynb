{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c339f0-db31-4ba5-8c53-4c95612bd927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from MIL.mil_utils import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c956ac9-3445-4f96-ab7a-8dc1a964b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "env = 'CCHS'\n",
    "id_feat = 'VisitIdentifier'\n",
    "sel_feats = ['SystolicBP', 'MAP', 'RespiratoryRate', 'PulseOx', 'HeartRate', 'Temperature', \n",
    "             'WBC', 'BiliRubin', 'BUN', 'Lactate', 'Creatinine', 'Platelet', 'Bands', 'FIO2']\n",
    "action_dim = 2 \n",
    "\n",
    "max_sel_feats = ['max_' + i for i in sel_feats]\n",
    "min_sel_feats = ['min_' + i for i in sel_feats]\n",
    "all_feats = sel_feats + max_sel_feats + min_sel_feats\n",
    "state_dim = len(all_feats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f63675-fbd9-43fa-985f-106338375430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "df = pd.read_csv('../baseline/example/cchs_sample_data.csv')\n",
    "vid_list = sorted(np.unique(df.VisitIdentifier))\n",
    "\n",
    "# Get the list of dataframes\n",
    "df_list = []\n",
    "vid_list = sorted(np.unique(df.VisitIdentifier))\n",
    "for vid in vid_list: \n",
    "    df_list.append(df.loc[df.VisitIdentifier == vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030f9ff2-777a-4cef-a495-719cdf6ca9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XiYang\\miniforge3\\envs\\themes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['latent_intention_network/dense/kernel', 'latent_intention_network/dense/bias', 'latent_intention_network/dense_1/kernel', 'latent_intention_network/dense_1/bias', 'latent_intention_network/dense_2/kernel', 'latent_intention_network/dense_2/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Generator Loss = 0.00897879246622324, Discriminator Loss = 0.02106037177145481\n",
      "Epoch 2: Generator Loss = 0.008957655169069767, Discriminator Loss = 0.019060995429754257\n",
      "Epoch 4: Generator Loss = 0.008949297480285168, Discriminator Loss = 0.01636641100049019\n",
      "Epoch 6: Generator Loss = 0.008946222253143787, Discriminator Loss = 0.012730655260384083\n",
      "Epoch 8: Generator Loss = 0.008920262567698956, Discriminator Loss = 0.008750165812671185\n",
      "Epoch 10: Generator Loss = 0.008903169073164463, Discriminator Loss = 0.005507076159119606\n",
      "Epoch 12: Generator Loss = 0.008886197581887245, Discriminator Loss = 0.003309172810986638\n",
      "Epoch 14: Generator Loss = 0.008869818411767483, Discriminator Loss = 0.002018131548538804\n",
      "Epoch 16: Generator Loss = 0.008858912624418736, Discriminator Loss = 0.001273058820515871\n",
      "Epoch 18: Generator Loss = 0.008824581280350685, Discriminator Loss = 0.0008439882658421993\n",
      "Performance Measurements:\n",
      "Confusion matrix: \n",
      " [[ 82 337]\n",
      " [  6 172]]\n",
      "Accuracy:  0.42546063651591287\n",
      "Recall:  0.9662921348314607\n",
      "Precision:  0.3379174852652259\n",
      "F-score:  0.5007278020378457\n",
      "AUC:  0.4764219248612266\n",
      "APR:  0.5111281667935472\n",
      "Jaccard Score:  0.3339805825242718\n"
     ]
    }
   ],
   "source": [
    "seed_rp_results = []\n",
    "for rp_idx in range(1): \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Get the training and testing index\n",
    "    tr_idx, te_idx = train_test_split(np.arange(len(df_list)), test_size=0.2, random_state=rp_idx)\n",
    "    \n",
    "    # Slice the training & testing data\n",
    "    tr_df = [df_list[i] for i in tr_idx]\n",
    "    te_df = [df_list[i] for i in te_idx]\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Get the expert trajectories\n",
    "    tr_states = list(itertools.chain.from_iterable([[row for row in np.array(tr_df[i][all_feats])] for i in range(len(tr_df))]))\n",
    "    tr_actions = list(itertools.chain.from_iterable([[row for row in np.array(tr_df[i]['Action'])] for i in range(len(tr_df))]))\n",
    "\n",
    "    # Augment the training data\n",
    "    pos_idx = [i for i in range(len(tr_actions)) if tr_actions[i] == 1]\n",
    "    aug_tr_states = [tr_states[i] for i in pos_idx] * 2\n",
    "    aug_tr_actions = [tr_actions[i] for i in pos_idx] * 2\n",
    "    tr_states.extend(aug_tr_states)\n",
    "    tr_actions.extend(aug_tr_actions)\n",
    "    \n",
    "    suf_idx = np.arange(len(tr_states))\n",
    "    random.seed(42)\n",
    "    random.shuffle(suf_idx)\n",
    "    \n",
    "    tr_states = [tr_states[i] for i in suf_idx]\n",
    "    tr_actions = [tr_actions[i] for i in suf_idx]\n",
    "    tr_actions = [np.array([.8, 0]) if x == 0 else np.array([0, .8]) for x in tr_actions]\n",
    "    expert_data = [(s,a) for s,a in zip(tr_states, tr_actions)]\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Training the MIL model\n",
    "    num_intentions = 3 # Latent variable dimension (context)\n",
    "    epochs = 20 #1000\n",
    "    batch_size = 64\n",
    "    \n",
    "    # Create models\n",
    "    latent_intention_network = LatentIntentionNetwork(num_intentions, num_intentions)\n",
    "    generator = Generator(state_dim, action_dim, latent_intention_network)\n",
    "    discriminator = Discriminator(state_dim, action_dim) #, num_intentions)\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_g = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    optimizer_d = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    \n",
    "    # Train the InfoGAIL model\n",
    "    generator, discriminator = train_info_gail(expert_data, generator, discriminator, optimizer_g, optimizer_d, \n",
    "                                               num_intentions, epochs=epochs, batch_size=batch_size, print_interval=2)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Testing the MIL model\n",
    "    # Get the testing trajectories\n",
    "    te_states = list(itertools.chain.from_iterable([[row for row in np.array(te_df[i][all_feats])] for i in range(len(te_df))]))\n",
    "    te_actions = list(itertools.chain.from_iterable([[row for row in np.array(te_df[i]['Action'])] for i in range(len(te_df))]))\n",
    "    te_actions = [np.array([.8, 0]) if x == 0 else np.array([0, .8]) for x in te_actions]\n",
    "    test_data = [(s,a) for s,a in zip(te_states, te_actions)]\n",
    "\n",
    "    # Evaluate the model\n",
    "    true_act, pred_act, pred_prob = [], [], []\n",
    "    avg_patterns = 'binary'\n",
    "    \n",
    "    for sa_idx in range(len(te_states)):\n",
    "        z = sample_latent_variable(num_intentions)  # Sample a latent variable for each state\n",
    "        latent_intention_probs = latent_intention_network(z)  # Get intention probabilities\n",
    "        generated_action = generator(te_states[sa_idx], z, latent_intention_probs)  # Get the action from the generator\n",
    "        \n",
    "        pred_prob.append(tf.nn.softmax(generated_action).numpy()[0])\n",
    "        pred_act.append(np.argmax(generated_action.numpy()[0]))\n",
    "        true_act.append(np.argmax(te_actions[sa_idx]))\n",
    "    \n",
    "    seed_rp_results.append(overall_eval(true_act, pred_act, pred_prob, 2, avg_patterns=avg_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ec7f6-07ab-478d-a956-f06925e9c805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3578eb-cd45-4f5d-9d40-4cf2092ca66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1737254-95c3-42dc-ab0f-9b7d6333a7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803efd7d-4eba-491c-9511-c6ba16ac722a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d6e09-d4fa-4e1b-af47-45473d5a2ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd8bec-d9b9-4b93-9b77-d91ff11d8a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
