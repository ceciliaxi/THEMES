{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-pepper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import mixture\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb8ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Projects\\Prior\\IRL\\baseline\\AHIL\\agent\\__open__.py:6: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XiYang\\miniforge3\\envs\\themes\\lib\\site-packages\\stable_baselines\\__init__.py:32: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from env import CustomizeEnv\n",
    "from AHIL.ahil_utils import *\n",
    "from AHIL.EM_EDM_utils import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd117e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NUM_STEPS_TRAIN': 500, 'EMEDM_LLH_THRES_CONV': 1e-05, 'EMEDM_LLH_THRES_DES': 10000.0, 'BATCH_SIZE': 64, 'MLP_WIDTHS': 32, 'EMEDM_ITERS': 10, 'EMEDM_nrCl': 3, 'ENV': 'CCHS', 'GYM_ENV': ['CartPole-v1', 'Acrobot-v1', 'MountainCar-v0'], 'ADAM_ALPHA': 0.001, 'ADAM_BETAS': [0.9, 0.999], 'SGLD_BUFFER_SIZE': 10000, 'SGLD_LEARN_RATE': 0.001, 'SGLD_NOISE_COEF': 0.01, 'SGLD_NUM_STEPS': 20, 'SGLD_REINIT_FREQ': 0.05, 'SAMPLE_BUFFER': 'balanced', 'EMEDM_BETA': 0.5, 'EMEDM_CLUSTER_THRES': 0, 'BASE_PATH': '', 'MODEL_LEARNER': 'EDM'}\n"
     ]
    }
   ],
   "source": [
    "base_path = '' \n",
    "args = {'env_name': 'CCHS', 'trial': 1} \n",
    "nrCl_dict = {'CCHS': 3} # Cluster Number \n",
    "\n",
    "# Whether to train the model/load from file\n",
    "training_mode = True\n",
    "save_model_result = True ### Whether to save the model\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------\n",
    "# Load the config file\n",
    "config_path = 'example/cchs_para_config.npy'\n",
    "config = np.load(config_path, allow_pickle=True).item()\n",
    "\n",
    "# Update the config values if necessary\n",
    "config['NUM_STEPS_TRAIN'] = 500\n",
    "config['BATCH_SIZE'] = 64\n",
    "config['MLP_WIDTHS'] = 32\n",
    "\n",
    "config['EMEDM_nrCl'] = nrCl_dict[args ['env_name']]\n",
    "config['SGLD_BUFFER_SIZE'] = 10000\n",
    "config['SGLD_LEARN_RATE'] = 1e-3\n",
    "\n",
    "config['SAMPLE_BUFFER'] = 'balanced' # 'random'/'stratified'/'balanced'\n",
    "config['BASE_PATH'] = base_path\n",
    "config['MODEL_LEARNER'] = 'EDM'\n",
    "print(config)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "avg_patterns = 'binary' # 'binary'/'weighted'/'micro'/'macro'\n",
    "init_mode = 'random' # Initialization method: 'random'/'dtw'/'kmeans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34142d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3391ef-c023-4268-b4b3-c8f05d4580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "env = 'CCHS'\n",
    "id_feat = 'VisitIdentifier'\n",
    "sel_feats = ['SystolicBP', 'MAP', 'RespiratoryRate', 'PulseOx', 'HeartRate', 'Temperature', \n",
    "             'WBC', 'BiliRubin', 'BUN', 'Lactate', 'Creatinine', 'Platelet', 'Bands', 'FIO2']\n",
    "act_num = 2\n",
    "\n",
    "max_sel_feats = ['max_' + i for i in sel_feats]\n",
    "min_sel_feats = ['min_' + i for i in sel_feats]\n",
    "all_feats = sel_feats + max_sel_feats + min_sel_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2507b2f9-c641-443f-874c-ba46ea220d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "df = pd.read_csv('../baseline/example/cchs_sample_data.csv')\n",
    "vid_list = sorted(np.unique(df.VisitIdentifier))\n",
    "\n",
    "# Get the list of dataframes\n",
    "df_list = []\n",
    "vid_list = sorted(np.unique(df.VisitIdentifier))\n",
    "for vid in vid_list: \n",
    "    df_list.append(df.loc[df.VisitIdentifier == vid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a896678d-cabc-4cdc-b9b6-26d5055bbf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XiYang\\miniforge3\\envs\\themes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=11.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Num of Partitioned trajs:  1580\n",
      "* Sub-traj for each cluster: \n",
      "  -  {0: 545, 1: 278, 2: 43, 3: 88, 4: 78, 5: 548}\n",
      "Cluster Idx: 0\n",
      "** Applying EM-EDM to learn policies... \n",
      "*** Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:19<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed to 1 cluster at 0-th iteration\n",
      "Cluster Idx: 1\n",
      "** Applying EM-EDM to learn policies... \n",
      "*** Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:15<00:00, 31.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed to 1 cluster at 0-th iteration\n",
      "Cluster Idx: 2\n",
      "** Applying EM-EDM to learn policies... \n",
      "*** Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed to 1 cluster at 0-th iteration\n",
      "Cluster Idx: 3\n",
      "** Applying EM-EDM to learn policies... \n",
      "*** Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:17<00:00, 29.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed to 1 cluster at 0-th iteration\n",
      "Cluster Idx: 4\n",
      "** Applying EM-EDM to learn policies... \n",
      "*** Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed to 1 cluster at 0-th iteration\n",
      "Cluster Idx: 5\n",
      "** Applying EM-EDM to learn policies... \n",
      "*** Iteration:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:16<00:00, 30.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collapsed to 1 cluster at 0-th iteration\n",
      "* Num of Partitioned trajs:  358\n",
      "* Sub-traj for each cluster: \n",
      "  -  {0: 118, 1: 51, 2: 21, 3: 28, 4: 14, 5: 126}\n",
      "Performance Measurements:\n",
      "Confusion matrix: \n",
      " [[173  26]\n",
      " [114  11]]\n",
      "Accuracy:  0.5679012345679012\n",
      "Recall:  0.088\n",
      "Precision:  0.2972972972972973\n",
      "F-score:  0.13580246913580246\n",
      "AUC:  0.6408643216080402\n",
      "APR:  0.6080200507597875\n",
      "Jaccard Score:  0.0728476821192053\n"
     ]
    }
   ],
   "source": [
    "seed_rp_results = [] \n",
    "\n",
    "for rp_idx in range(1): \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Get the training and testing index\n",
    "    tr_idx, te_idx = train_test_split(np.arange(len(df_list)), test_size=0.2, random_state=rp_idx)\n",
    "    \n",
    "    # Slice the training & testing data\n",
    "    tr_df = [df_list[i] for i in tr_idx]\n",
    "    te_df = [df_list[i] for i in te_idx]\n",
    "    \n",
    "    # Sub-clustering the training data by GMM\n",
    "    tr_gmm_data = np.array(pd.concat([tr_df[i][sel_feats] for i in range(len(tr_df))]))\n",
    "    \n",
    "    gmm_clusters = 6\n",
    "    gmm = mixture.GaussianMixture(n_components=gmm_clusters, max_iter=10000)\n",
    "    gmm.fit(tr_gmm_data)\n",
    "    tr_gmm_pred = gmm.predict(tr_gmm_data)\n",
    "\n",
    "    # Get the sub-trajectories\n",
    "    tr_sub_demos, tr_sub_demos_lab = subTrajectoriesbyCluster(tr_df, all_feats, tr_gmm_pred, clus_num=gmm_clusters)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Training the model for each cluster\n",
    "    student_dict = {}\n",
    "    tr_sub_trajs, tr_sub_returns = tr_sub_demos['trajs'], tr_sub_demos['returns']\n",
    "    \n",
    "    for clus_idx in range(gmm_clusters): \n",
    "        print('Cluster Idx:', clus_idx)\n",
    "        \n",
    "        # Get the demos within the current cluster\n",
    "        clus_tr_sub_trajs, clus_tr_sub_returns = [], []\n",
    "        for i in range(len(tr_sub_demos_lab)): \n",
    "            if tr_sub_demos_lab[i] == clus_idx: \n",
    "                clus_tr_sub_trajs.append(tr_sub_trajs[i])\n",
    "                clus_tr_sub_returns.append(tr_sub_returns[i])\n",
    "        \n",
    "        clus_sub_demos = {}\n",
    "        clus_sub_demos['trajs'], clus_sub_demos['returns'] = clus_tr_sub_trajs, clus_tr_sub_returns\n",
    "    \n",
    "        # Training process \n",
    "        # EM-EDM for each cluster(cluster number = 1) ==> Apply EDM for each cluster\n",
    "        init_seeds = 42 \n",
    "        nrCl = 1\n",
    "        config['ENV_VOLUME_PATH'] = base_path + 'example/' + args['env_name']\n",
    "        config['CLUS_BASE_PATH'] = config['ENV_VOLUME_PATH'] \n",
    "        \n",
    "        # Load the expert trajectories\n",
    "        clus_teachers = clus_sub_demos \n",
    "        clus_trajs, clus_returns = clus_teachers['trajs'], clus_teachers['returns']\n",
    "        traj_num = len(clus_trajs)\n",
    "    \n",
    "        model_folder = 'example/AHIL_models/' + args['env_name']\n",
    "        model_path = ''.join([model_folder, '/AHIL', '_', str(traj_num), '_fold_', str(rp_idx), '.sav'])\n",
    "        Path(model_folder).mkdir(parents = True, exist_ok = True)\n",
    "        config['MODEL_PATH'] = model_path\n",
    "    \n",
    "        # Learn the different policies by EM-EDM\n",
    "        print('** Applying EM-EDM to learn policies... ')\n",
    "        seed_tmp_results, seed_tmp_jaccard = [], []\n",
    "        \n",
    "        if training_mode:\n",
    "            # Initialize the clusters \n",
    "            pred_labs = InitEMEDMClusters(clus_trajs, nrCl, len(tr_df), init_mode=init_mode, \n",
    "                                          DTW_thres=1e5, max_iter=10, init_seed=init_seeds)\n",
    "            # ---------------------------------------------------------\n",
    "            # EM-EDM to cluster the trajectories            \n",
    "            student_list, rho, LLH, nrCl, pred_labs, pred_probs = EMEDMWarped(clus_teachers, nrCl, pred_labs, \n",
    "                                                                              config, decay_expert=False, verbo=False)\n",
    "            # ---------------------------------------------------------\n",
    "            # Save the learnd models (policies per cluster) to file\n",
    "            if save_model_result: \n",
    "                pickle.dump((student_list, rho), open(model_path, 'wb'))\n",
    "        else: \n",
    "            student_list, rho = pickle.load(open(model_path, 'rb'))\n",
    "    \n",
    "        student_dict[clus_idx] = student_list[0]\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Sub-clustering the testing data by GMM\n",
    "    te_gmm_data = np.array(pd.concat([te_df[i][sel_feats] for i in range(len(te_df))]))\n",
    "    te_gmm_pred = gmm.predict(te_gmm_data)\n",
    "    te_sub_demos, te_sub_demos_lab = subTrajectoriesbyCluster(te_df, all_feats, te_gmm_pred, clus_num=gmm_clusters)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    # Model evaluation\n",
    "    true_act, pred_act, pred_prob = [], [], []\n",
    "    # For each subtrajectory find the corresponding policy\n",
    "    for idx in range(len(te_sub_demos_lab)): \n",
    "        clusIdx = te_sub_demos_lab[idx]\n",
    "        if clusIdx in student_dict.keys(): \n",
    "            pass\n",
    "        else: \n",
    "            print('RANDOMLY SELECT A CLUSTER TO FIT ...')\n",
    "            random.seed(idx + rp_idx)\n",
    "            clusIdx = random.sample(student_dict.keys(), 1)[0]\n",
    "    \n",
    "        # For each state-action pair\n",
    "        for sa_idx in range(len(te_sub_demos['trajs'][idx])):\n",
    "            tmp_act, _, tmp_qvalue = student_dict[clusIdx].select_action(te_sub_demos['trajs'][idx][sa_idx][0])\n",
    "            # Apply softmax to Q-values to get action probabilities\n",
    "            action_probabilities = F.softmax(torch.tensor(tmp_qvalue), dim=0).numpy()\n",
    "            \n",
    "            true_act.append(te_sub_demos['trajs'][idx][sa_idx][1][0])\n",
    "            pred_act.append(tmp_act)\n",
    "            pred_prob.append(action_probabilities)\n",
    "\n",
    "    seed_rp_results.append(overall_eval(true_act, pred_act, pred_prob, 2, avg_patterns=avg_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730dc757-69ad-4833-b78d-83f7f350db60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7d0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1036397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
